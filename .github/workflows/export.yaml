name: Build & Export (tiles + hashes + runs + skip-unchanged)

permissions:
  id-token: write
  contents: read

on:
  push:
    branches: [ "master" ]
  workflow_dispatch: {}

env:
  TILE: 512
  OUT_DIR: data/minimaps
  TILE_DIR: data/tiles
  S3_BUCKET: s3://aurora-webtilesbucket
  S3_PREFIX: ''
  PUBLIC_BASE: https://tiles.armrha.org/

jobs:
  export:
    runs-on: ubuntu-24.04
    timeout-minutes: 90

    steps:
      - uses: actions/checkout@v4

      - name: Compute S3 prefix (date/branch/run/sha)
        id: s3p
        shell: bash
        run: |
          set -euo pipefail
          day=$(date -u +%F)
          sha7=${GITHUB_SHA::7}
          echo "S3_PREFIX=aurora-tiles/${day}/${GITHUB_REF_NAME}/${GITHUB_RUN_ID}-${sha7}" >> "$GITHUB_ENV"
          echo "Will upload under: ${S3_BUCKET%/}/${S3_PREFIX}/"

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Build SpacemanDMM
        run: |
          set -eux
          git clone --depth=1 https://github.com/SpaceManiac/SpacemanDMM /tmp/SpacemanDMM
          cd /tmp/SpacemanDMM && cargo build --release --bin dmm-tools
          echo "/tmp/SpacemanDMM/target/release" >> $GITHUB_PATH

      - name: Install ImageMagick + jq + AWS CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y imagemagick jq unzip
          curl -sSL "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o awscliv2.zip
          unzip -q awscliv2.zip
          sudo ./aws/install --update
          /usr/bin/convert -version | head -n 1
          aws --version

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::676973469311:role/AuroraWebTilesBucketOIDC
          aws-region: us-west-2

      # --- Discover previous run prefix (unchanged from you) ---
      - name: Fetch previous runs.json
        id: prev
        shell: bash
        run: |
          set -euo pipefail
          PREV_PREFIX=""
          tmp_runs=$(mktemp)
          if aws s3 cp "${S3_BUCKET%/}/aurora-tiles/runs.json" "$tmp_runs" --only-show-errors; then
            PREV_PREFIX=$(jq -r '.runs[0].prefix // empty' "$tmp_runs")
            echo "Previous prefix: $PREV_PREFIX"
          else
            echo "No previous runs.json found."
          fi
          echo "prev_prefix=$PREV_PREFIX" >> "$GITHUB_OUTPUT"
      
      # --- Download previous map_hashes.json if it exists ---
      - name: Fetch previous map_hashes.json
        id: prevhash
        if: steps.prev.outputs.prev_prefix != ''
        shell: bash
        run: |
          set -euo pipefail
          if aws s3 cp "${S3_BUCKET%/}/${{ steps.prev.outputs.prev_prefix }}/data/tiles/map_hashes.json" prev_map_hashes.json --only-show-errors; then
            echo "prev_hash_file=prev_map_hashes.json" >> "$GITHUB_OUTPUT"
          else
            echo "prev_hash_file=" >> "$GITHUB_OUTPUT"
            echo "No previous map_hashes.json found at prefix."
          fi

      - name: Compute map_hashes.json (git blob hashes of .dmm)
        id: curhash
        shell: bash
        run: |
          set -Eeuo pipefail
          set -x
      
          # Seed file
          jq -n --arg commit "$GITHUB_SHA" --arg gen "$(date -u +%FT%TZ)" \
            '{commit_full:$commit, generated_at:$gen, maps:{}}' > map_hashes.json
      
          # Build a null-delimited, sorted list of .dmm files from both roots (if they exist)
          tmp_list="$(mktemp)"
          {
            [ -d maps  ]  && find maps  -type f -name '*.dmm' -print0 || true
            [ -d _maps ] && find _maps -type f -name '*.dmm' -print0 || true
          } | LC_ALL=C sort -z > "$tmp_list"
      
          # If no files, fail early with a clear message
          if [ ! -s "$tmp_list" ]; then
            echo "No .dmm maps found in ./maps or ./_maps" >&2
            exit 1
          fi
      
          # Update JSON in a temp file to avoid partial writes
          tmp_json="$(mktemp)"
          cp map_hashes.json "$tmp_json"
      
          # Iterate null-delimited files
          set +x
          count=0
          while IFS= read -r -d '' f; do
            rel="${f#./}"
            h="$(git hash-object "$f")"
            jq --arg p "$rel" --arg h "$h" '.maps[$p]=$h' "$tmp_json" > "$tmp_json.next"
            mv "$tmp_json.next" "$tmp_json"
            count=$((count+1))
          done < "$tmp_list"
          set -x
      
          mv "$tmp_json" map_hashes.json
      
          # Optional: quick debug summary so we can sanity-check the shape
          echo "::group::DEBUG map_hashes.json (current)"
          jq -r '
            "generated_at: \(.generated_at // "n/a")",
            "commit_full: \(.commit_full // "n/a")",
            "MAP COUNT: \((.maps // {}) | keys | length)",
            "SAMPLE KEYS:",
            ((.maps // {}) | keys | sort | .[0:20][] // empty)
          ' map_hashes.json
          echo "::endgroup::"
          echo "cur_hash_file=map_hashes.json" >> "$GITHUB_OUTPUT"
      - name: Compare map hashes and build change lists
        id: diff
        shell: bash
        run: |
          set -Eeuo pipefail
      
          CUR="${{ steps.curhash.outputs.cur_hash_file }}"
          PREV="${{ steps.prevhash.outputs.prev_hash_file }}"
          : "${CUR:?cur_hash_file missing}"
      
          touch changed_maps.txt
      
          if [ -n "${PREV:-}" ] && [ -f "$PREV" ]; then
            echo "Hash diff"
            python3 .github/scripts/compare_map_hashes.py "$PREV" "$CUR" \
              --changed-out changed_maps.txt --summary-out hash_diff_summary.txt | tee hashdiff.out
          else
            # ---- FIRST RUN: treat all as changed ----
            jq -r '.maps | keys[]' "$CUR" > changed_maps.txt
            printf "added:0 removed:0 changed:%s same:0\n" "$(wc -l < changed_maps.txt | tr -d ' ')" > hash_diff_summary.txt
            printf "any_changed=true\nchanged_count=%s\nadded_count=0\nremoved_count=0\n" "$(wc -l < changed_maps.txt | tr -d ' ')" > hashdiff.out
          fi
      
          echo "---- summary ----"
          cat hash_diff_summary.txt
      
          # Build CHANGED JSON [{path,id}]
          CHANGED='[]'
          while IFS= read -r path; do
            [ -z "$path" ] && continue
            id="$(echo "$path" | sed -E 's/\.dmm$//; s/[^A-Za-z0-9._-]+/_/g')"
            CHANGED=$(jq -c --arg path "$path" --arg id "$id" '. + [{path:$path,id:$id}]' <<<"$CHANGED")
          done < changed_maps.txt
      
          # Build UNCHANGED JSON via set difference (will be [] on first run)
          tmp_all=$(mktemp); jq -r '.maps | keys[]' "$CUR" | LC_ALL=C sort > "$tmp_all"
          tmp_chg=$(mktemp); LC_ALL=C sort changed_maps.txt > "$tmp_chg"
          tmp_unchanged=$(mktemp); comm -23 "$tmp_all" "$tmp_chg" > "$tmp_unchanged"
      
          UNCHANGED='[]'
          while IFS= read -r path; do
            [ -z "$path" ] && continue
            id="$(echo "$path" | sed -E 's/\.dmm$//; s/[^A-Za-z0-9._-]+/_/g')"
            UNCHANGED=$(jq -c --arg path "$path" --arg id "$id" '. + [{path:$path,id:$id}]' <<<"$UNCHANGED")
          done < "$tmp_unchanged"
      
          {
            echo "changed<<JSON"
            jq -c . <<<"$CHANGED"
            echo "JSON"
            echo "unchanged<<JSON"
            jq -c . <<<"$UNCHANGED"
            echo "JSON"
          } >> "$GITHUB_OUTPUT"
      
          # Bubble comparator flags to outputs (any_changed, counts)
          while IFS='=' read -r k v; do
            echo "$k=$v" >> "$GITHUB_OUTPUT"
          done < hashdiff.out
          # prev keys (only if a prev hash exists)
          tmp_prev=$(mktemp)
          if [ -n "${PREV:-}" ] && [ -f "$PREV" ]; then
            jq -r '.maps | keys[]' "$PREV" | LC_ALL=C sort > "$tmp_prev"
          else
            : > "$tmp_prev"
          fi
          
          # removed = prev - current
          tmp_removed=$(mktemp)
          comm -13 "$tmp_all" "$tmp_prev" > "$tmp_removed" || true
          
          REMOVED='[]'
          while IFS= read -r path; do
            [ -z "$path" ] && continue
            id="$(echo "$path" | sed -E 's/\.dmm$//; s/[^A-Za-z0-9._-]+/_/g')"
            REMOVED=$(jq -c --arg path "$path" --arg id "$id" '. + [{path:$path,id:$id}]' <<<"$REMOVED")
          done < "$tmp_removed"
      
          # Keep prev prefix available for the synthesize step
          echo "prev_prefix=${{ steps.prev.outputs.prev_prefix }}" >> "$GITHUB_OUTPUT"
          
      # --- Render ONLY changed/new maps ---
      - name: Render & tile changed maps
        if: steps.diff.outputs.changed != '[]'
        shell: bash
        env:
          CHANGED: ${{ steps.diff.outputs.changed }}
        run: |
          set -euo pipefail
          shopt -s nullglob
          mkdir -p "$OUT_DIR" "$TILE_DIR"

          jq -c '.[]' <<< "$CHANGED" | while read -r m; do
            rel=$(jq -r '.path' <<< "$m")
            map_id=$(jq -r '.id'   <<< "$m")
            echo "::group::Rendering (changed) $rel  (id=$map_id)"

            out_map="$OUT_DIR/$map_id"
            rm -rf "$out_map"; mkdir -p "$out_map"
            dmm-tools minimap "$rel" -o "$out_map"

            rm -rf "$TILE_DIR/$map_id"; mkdir -p "$TILE_DIR/$map_id"
            zmeta='[]'
            for png in "$out_map"/*.png; do
              base="$(basename "$png")"
              z="${base%.*}"; z="${z##*-}"
              out="$TILE_DIR/$map_id/z$z"
              mkdir -p "$out"

              /usr/bin/convert "$png" \
                -colorspace sRGB -alpha on -type TrueColorAlpha -background none \
                -crop ${TILE}x${TILE} \
                -set filename:tile "%[fx:floor(page.y/${TILE})],%[fx:floor(page.x/${TILE})]" \
                +repage -gravity northwest -extent ${TILE}x${TILE} \
                +adjoin "$out/%[filename:tile].png"

              w=$(/usr/bin/identify -format '%w' "$png")
              h=$(/usr/bin/identify -format '%h' "$png")
              cols=$(( (w + TILE - 1) / TILE ))
              rows=$(( (h + TILE - 1) / TILE ))
              # zmeta: single-line jq, numerics via --argjson
              zmeta="$(jq -c \
                --argjson z "$z" \
                --arg label "$(basename "$map_id") z$z" \
                --argjson w "$w" --argjson h "$h" \
                --argjson rows "$rows" --argjson cols "$cols" \
                --argjson tile "$TILE" \
                '. + [{z:$z, label:$label, width_px:$w, height_px:$h, rows:$rows, cols:$cols, tile_size:$tile}]' <<<"$zmeta")"
            done

            jq -n \
              --arg id "$map_id" \
              --arg tiles_base "${PUBLIC_BASE%/}/${S3_PREFIX}/data/tiles/$map_id" \
              --arg minimaps_base "${PUBLIC_BASE%/}/${S3_PREFIX}/data/minimaps/$map_id" \
              --arg source_dmm "$rel" \
              --argjson z_levels "$zmeta" \
              '{map_id:$id, source_dmm:$source_dmm, tiles_base:$tiles_base, minimaps_base:$minimaps_base, z_levels:$z_levels}' \
              > "$TILE_DIR/$map_id/index.json"

            echo "::endgroup::"
          done

      # --- Synthesize per-map indexes for UNCHANGED maps (point to previous tiles) ---
      - name: Synthesize unchanged per-map indexes (point to previous run)
        if: steps.diff.outputs.prev_prefix != '' && steps.diff.outputs.unchanged != '[]'
        shell: bash
        env:
          UNCHANGED:  ${{ steps.diff.outputs.unchanged }}
          PREV_PREFIX: ${{ steps.diff.outputs.prev_prefix }}
        run: |
          set -euo pipefail
          shopt -s nullglob
          mkdir -p "$TILE_DIR"

          jq -c '.[]' <<< "$UNCHANGED" | while read -r m; do
            rel=$(jq -r '.path' <<< "$m")
            map_id=$(jq -r '.id'   <<< "$m")
            echo "::group::Synthesizing (unchanged) $rel (id=$map_id)"
            mkdir -p "$TILE_DIR/$map_id"
            tmp=$(mktemp)
            aws s3 cp "${S3_BUCKET%/}/${PREV_PREFIX}/data/tiles/${map_id}/index.json" "$tmp" --only-show-errors
            jq --arg tb "${PUBLIC_BASE%/}/${PREV_PREFIX}/data/tiles/${map_id}" \
               '.tiles_base = $tb' "$tmp" > "$TILE_DIR/${map_id}/index.json"
            echo "::endgroup::"
          done
      - name: Synthesize removed per-map indexes (carry forward from previous run)
        if: steps.diff.outputs.any_changed == 'true' && steps.diff.outputs.prev_prefix != '' && steps.diff.outputs.removed != '[]'
        shell: bash
        env:
          REMOVED:     ${{ steps.diff.outputs.removed }}
          PREV_PREFIX: ${{ steps.diff.outputs.prev_prefix }}
          PUBLIC_BASE: ${{ env.PUBLIC_BASE }}
          TILE_DIR:    ${{ env.TILE_DIR }}
        run: |
          set -euo pipefail
          shopt -s nullglob
          mkdir -p "$TILE_DIR"
      
          jq -c '.[]' <<< "$REMOVED" | while read -r m; do
            rel=$(jq -r '.path' <<< "$m")
            map_id=$(jq -r '.id'   <<< "$m")
            echo "::group::Synthesizing (removed) $rel (id=$map_id)"
            mkdir -p "$TILE_DIR/$map_id"
            tmp=$(mktemp)
            # Pull the prior per-map index
            aws s3 cp "${S3_BUCKET%/}/${PREV_PREFIX}/data/tiles/${map_id}/index.json" "$tmp" --only-show-errors
            # Rewrite bases to point at the prev prefix; mark removed:true
            jq \
              --arg tb "${PUBLIC_BASE%/}/${PREV_PREFIX}/data/tiles/${map_id}" \
              --arg mb "${PUBLIC_BASE%/}/${PREV_PREFIX}/data/minimaps/${map_id}" \
              '.tiles_base = $tb | .minimaps_base = $mb | .removed = true' \
              "$tmp" > "$TILE_DIR/${map_id}/index.json"
            echo "::endgroup::"
          done
      # --- Per-run catalog over BOTH changed + synthesized indexes ---
      - name: Build per-run catalog (data/tiles/index.json)
        run: |
          set -euo pipefail
          shopt -s nullglob
          mkdir -p "$(dirname "$TILE_DIR/index.json")"
          tmp="$TILE_DIR/_maps.json"
          jq -s '.' "$TILE_DIR"/*/index.json > "$tmp"
          jq -n \
            --arg generated "$(date -u +%FT%TZ)" \
            --argjson tile "$TILE" \
            --slurpfile maps "$tmp" \
            '{generated_at:$generated, tile_size:$tile, maps:$maps[0]}' \
            > "$TILE_DIR/index.json"
          rm -f "$tmp"

      - name: Diff counts (debug)
        run: |
          echo "Changed count: $(jq -r 'length' <<< '${{ steps.diff.outputs.changed }}')"
          echo "Unchanged count: $(jq -r 'length' <<< '${{ steps.diff.outputs.unchanged }}')"

      # --- Upload tiles (CHANGED only) ---
      - name: Upload tiles (changed only)
        if: steps.diff.outputs.changed != '[]'
        shell: bash
        env:
          CHANGED: ${{ steps.diff.outputs.changed }}
        run: |
          set -euo pipefail
          jq -c '.[]' <<< "$CHANGED" | while read -r m; do
            map_id=$(jq -r '.id' <<< "$m")
            aws s3 sync "$TILE_DIR/$map_id" "${S3_BUCKET%/}/${S3_PREFIX}/data/tiles/$map_id" \
              --only-show-errors \
              --exclude "*" --include "*.png" \
              --cache-control "public, max-age=31536000, immutable"
          done

      - name: Upload JSON indices (per-map + per-run + hashes)
        run: |
          find "$TILE_DIR" -type f -name '*.json' -print0 | while IFS= read -r -d '' f; do
            aws s3 cp "$f" "${S3_BUCKET%/}/${S3_PREFIX}/${f}" \
              --only-show-errors \
              --content-type "application/json" \
              --cache-control "public, max-age=60, must-revalidate"
          done
          aws s3 cp map_hashes.json "${S3_BUCKET%/}/${S3_PREFIX}/data/tiles/map_hashes.json" \
            --only-show-errors \
            --content-type "application/json" \
            --cache-control "public, max-age=60, must-revalidate"

      # --- Update global runs.json history ---
      - name: Update global runs.json
        shell: bash
        run: |
          set -euo pipefail
          tiles=$(find "$TILE_DIR" -type f -name '*.png' | wc -l | tr -d ' ')
          bytes=$(du -sb "$TILE_DIR" | awk '{print $1}')
          maps=$(find "$TILE_DIR" -mindepth 2 -maxdepth 2 -name index.json | wc -l | tr -d ' ')
          now=$(date -u +%FT%TZ)
          sha7=${GITHUB_SHA::7}

          entry=$(jq -n \
            --arg prefix "$S3_PREFIX" \
            --arg branch "${GITHUB_REF_NAME}" \
            --arg commit "$sha7" \
            --arg now "$now" \
            --argjson tiles "$tiles" \
            --argjson bytes "$bytes" \
            --argjson maps "$maps" \
            '{prefix:$prefix, branch:$branch, commit:$commit, generated_at:$now, map_count:$maps, tile_count:$tiles, bytes:$bytes}')

          tmp_old=$(mktemp)
          if aws s3 cp "${S3_BUCKET%/}/aurora-tiles/runs.json" "$tmp_old" --only-show-errors; then
            jq --argjson new "$entry" '
              .generated_at = (now | todateiso8601)
              | .runs = ([ $new ] + (.runs // []))
              | .runs = ((reduce .runs[] as $r ({}; .[$r.prefix] = $r)) | to_entries | map(.value))
              | .runs = (.runs | sort_by(.generated_at) | reverse)
            ' "$tmp_old" > runs.json
          else
            jq -n --arg gen "$now" --argjson tile "$TILE" --argjson new "$entry" \
              '{generated_at:$gen, tile_size:$tile, runs:[ $new ]}' > runs.json
          fi

          aws s3 cp runs.json "${S3_BUCKET%/}/aurora-tiles/runs.json" \
            --only-show-errors \
            --content-type "application/json" \
            --cache-control "public, max-age=60, must-revalidate"

      - name: Where to read catalogs
        run: |
          echo "Per-run catalog:"
          echo "  ${PUBLIC_BASE%/}/${S3_PREFIX}/data/tiles/index.json"
          echo "Global history:"
          echo "  ${PUBLIC_BASE%/}/aurora-tiles/runs.json"
