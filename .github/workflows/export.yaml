name: Build & Export (streamed tiles + compression)
permissions:
  id-token: write 
  contents: read

on:
  push:
    branches: [ "master" ]
  workflow_dispatch: {}
  
env:
  TILE: 512
  OUT_DIR: data/minimaps
  TILE_DIR: data/tiles
  ZIP_DIR: data/zips
  # Optional S3 destination like: s3://my-bucket/aurora-tiles
  S3_BUCKET: s3://aurora-webtilesbucket          # set in repo/env or Secrets if you want S3 upload

jobs:
  export:
    runs-on: ubuntu-24.04
    timeout-minutes: 90
    steps:
      - uses: actions/checkout@v4

      - name: Compute S3 prefix (date/branch/run/sha)
        id: s3p
        shell: bash
        run: |
          day=$(date -u +%F)                              # e.g. 2025-08-26
          sha7=${GITHUB_SHA::7}
          echo "S3_PREFIX=aurora-tiles/${day}/${GITHUB_REF_NAME}/${GITHUB_RUN_ID}-${sha7}" >> "$GITHUB_ENV"
          echo "Will upload under: ${S3_BUCKET%/}/${S3_PREFIX}/"
      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Build SpacemanDMM
        run: |
          set -eux
          git clone --depth=1 https://github.com/SpaceManiac/SpacemanDMM /tmp/SpacemanDMM
          cd /tmp/SpacemanDMM && cargo build --release --bin dmm-tools
          echo "/tmp/SpacemanDMM/target/release" >> $GITHUB_PATH
          dmm-tools --help | head -n 1 || true

      - name: Install tiling & compression tools
        run: |
          sudo apt-get update
          sudo apt-get install -y imagemagick pngquant jq
          /usr/bin/convert -version | head -n 1
          pngquant --version | head -n 1 || true

      # 1) Configure AWS creds via OIDC (preferred over static keys)
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          # Either assume a role…
          role-to-assume: arn:aws:iam::676973469311:role/AuroraWebTilesBucketOIDC
          aws-region: us-west-2
          # …or, if you must, use access keys stored as secrets:
          # aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          # aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      
      # 2) Install AWS CLI v2
      - name: Install AWS CLI v2
        run: |
          curl -sSL "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o awscliv2.zip
          sudo apt-get update && sudo apt-get install -y unzip
          unzip -q awscliv2.zip
          sudo ./aws/install --update
          aws --version

      - name: Render → Tile → Compress → Zip → (optional) Upload per map
        shell: bash
        env:
          MAP_GLOB_A: "maps/**/*.dmm"
          MAP_GLOB_B: "_maps/**/*.dmm"
        run: |
          set -euo nounset
          # Do NOT set -o pipefail so 'find | head' samples won't kill the job.
          shopt -s nullglob globstar

          mkdir -p "$OUT_DIR" "$TILE_DIR" "$ZIP_DIR"

          # Build a list of maps. Add/remove globs as needed.
          MAPS=()
          for m in $MAP_GLOB_A $MAP_GLOB_B; do
            [ -f "$m" ] && MAPS+=("$m")
          done

          if [ "${#MAPS[@]}" -eq 0 ]; then
            echo "No maps found under maps/** or _maps/**"; exit 1
          fi

          echo "Found ${#MAPS[@]} maps to process."

          for m in "${MAPS[@]}"; do
            rel="${m#./}"
            # map_id: safe identifier (path without .dmm, non-alnum → _)
            map_id="$(echo "$rel" | sed -E 's/\.dmm$//; s/[^A-Za-z0-9._-]+/_/g')"
            echo "::group::Processing $m  (id=$map_id)"

            # --- Render minimaps for this map into a temp subdir ---
            out_map="$OUT_DIR/$map_id"
            rm -rf "$out_map"; mkdir -p "$out_map"
            echo "Rendering: $m -> $out_map"
            dmm-tools minimap "$m" -o "$out_map"

            # --- Tile each Z image, compress, and accumulate per-map metadata ---
            rm -rf "$TILE_DIR/$map_id"; mkdir -p "$TILE_DIR/$map_id"
            zmeta='[]'

            for png in "$out_map"/*.png; do
              base="$(basename "$png")"
              z="${base%.*}"; z="${z##*-}"                      # YourMap-3.png -> 3
              out="$TILE_DIR/$map_id/z$z"
              mkdir -p "$out"

              # Crop into TILE×TILE tiles; top-left origin, name row,col.png; preserve RGBA; pad edges
              /usr/bin/convert "$png" \
                -colorspace sRGB -alpha on -type TrueColorAlpha -background none \
                -crop ${TILE}x${TILE} \
                -set filename:tile "%[fx:floor(page.y/${TILE})],%[fx:floor(page.x/${TILE})]" \
                +repage -gravity northwest -extent ${TILE}x${TILE} \
                +adjoin "$out/%[filename:tile].png"

              # We don't need the big Z PNG anymore
              rm -f "$png"

              # Lossy-ish palette quantization (huge wins for SS13 art); keep if larger
              pngquant --quality=70-95 --speed 1 --strip --force --skip-if-larger --ext .png "$out"/*.png || true

              # Gather z-level metadata
              w=$(/usr/bin/identify -format '%w' "$out/0,0.png" 2>/dev/null || /usr/bin/identify -format '%w' "$out/0,1.png" 2>/dev/null || echo 0)
              h=$(/usr/bin/identify -format '%h' "$out/0,0.png" 2>/dev/null || /usr/bin/identify -format '%h' "$out/0,1.png" 2>/dev/null || echo 0)
              # Compute rows/cols from a sentinel tile count (more robust: count files)
              cols=$(ls "$out"/*.png 2>/dev/null | sed -E 's@.*/([0-9]+),([0-9]+)\.png@\1@' | sort -nu | tail -n1)
              rows=$(ls "$out"/*.png 2>/dev/null | sed -E 's@.*/([0-9]+),([0-9]+)\.png@\2@' | sort -nu | tail -n1)
              if [ -z "$cols" ] || [ -z "$rows" ]; then
                # Fallback using image geometry
                imgw=$(/usr/bin/identify -format '%w' "$OUT_DIR/$map_id/$base" 2>/dev/null || echo 0)
                imgh=$(/usr/bin/identify -format '%h' "$OUT_DIR/$map_id/$base" 2>/dev/null || echo 0)
                cols=$(( (imgw + TILE - 1)/TILE ))
                rows=$(( (imgh + TILE - 1)/TILE ))
              else
                cols=$((cols+1))
                rows=$((rows+1))
              fi
              zmeta="$(jq -c \
                --arg z "$z" \
                --argjson rows "$rows" \
                --argjson cols "$cols" \
                --arg tile "$TILE" \
                '. + [{z:($z|tonumber), rows:$rows, cols:$cols, tile_size:($tile|tonumber)}]' \
                <<< "$zmeta")"
            done

            # Per-map index.json
            jq -n \
              --arg id "$map_id" \
              --arg tiles_base "data/tiles/$map_id" \
              --arg minimaps_base "data/minimaps/$map_id" \
              --arg source_dmm "$rel" \
              --argjson z_levels "$zmeta" \
              '{map_id:$id, source_dmm:$source_dmm, tiles_base:$tiles_base, minimaps_base:$minimaps_base, z_levels:$z_levels}' \
              > "$TILE_DIR/$map_id/index.json"

            # Zip the map tiles + per-map index; then free the tiles on disk
            ( cd "$TILE_DIR" && zip -qr "../zips/${map_id}.zip" "$map_id" )
            rm -rf "$TILE_DIR/$map_id"

            # Optionally upload to S3 and delete local zip to save space
            if [ -n "${S3_BUCKET}" ]; then
              dest="${S3_BUCKET%/}/${S3_PREFIX:+$S3_PREFIX/}${map_id}.zip"
              echo "Uploading to ${dest}"
              aws s3 cp "$ZIP_DIR/${map_id}.zip" "${dest}"
              rm -f "$ZIP_DIR/${map_id}.zip"
            fi

            # Cleanup rendered minimaps for this map to save space
            rm -rf "$out_map"
            echo "::endgroup::"
          done

          # Build top-level catalog for whatever remains locally (zips and/or tiles if you kept them)
          # (Tiles are zipped & removed; the catalog still helps downstream publishing.)
          maps_json=$(jq -n '[]')
          for z in "$ZIP_DIR"/*.zip; do
            [ -f "$z" ] || continue
            id="$(basename "$z" .zip)"
            maps_json="$(jq -c --arg id "$id" '. + [$id]' <<< "$maps_json")"
          done
          jq -n --arg generated "$(date -u +%FT%TZ)" --argjson maps "$maps_json" \
            '{generated_at:$generated, zipped_maps:$maps}' > "$ZIP_DIR/index.json"

      - name: Upload artifacts (remaining zips)
        if: ${{ env.S3_BUCKET == '' }}
        uses: actions/upload-artifact@v4
        with:
          name: aurora-tiles-zips-${{ github.sha }}
          path: |
            data/zips/*.zip
            data/zips/index.json
          if-no-files-found: error
